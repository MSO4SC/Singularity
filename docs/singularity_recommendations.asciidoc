= Singularity recommendations
CESGA
v0.1 (2017-09-20), <vsande at cesga dot es>
:toc:


== 1. Storage devices

=== 1.1 Bootstrap

In order to get access to _Finis Terrae II_ storage devices, we encourage you to check the existence of `/mnt` directory inside the container.
If this directory doesn't exists you can create it.

Although being a default directory in the OS hierarchy, we recommend to add the command `mkdir -p /mnt` to the `%post` section of your bootstrap definition file.

=== 1.2 Usage

To get a transparent access to _Finis Terrae 2_ storage devices while using _Singularity_, our recommendation is to bind-mount `/mnt` directory inside the container.

[source,shell]
    $ module purge
    $ module load singularity/2.3.2
    $ singularity exec -B /mnt $CONTAINER $COMMAND

== 2. Scratch directory

=== 1.1 Bootstrap

In order to get access to `/scratch` directories of the compute nodes, we encourage you to create this directory inside the container.

We recommend to add the command `mkdir -p /scratch` to the `%post` section of your bootstrap definition file.

=== 2.2 Usage

It's recommended to use a `scratch` folder to store temporal data while executing in a _Finis Terrae 2_ compute node.

The most transparent strategy is to bind-mount the `/scratch` host directory into the `/scratch` container directory.

[source,shell]
    $ module purge
    $ module load singularity/2.3.2
    $ singularity exec -B /scratch $CONTAINER $COMMAND

Other alternative is to bind-mount the `/scratch` host directory in other temporal directory like `/tmp`.

NOTE: By default at _Finis Terrae 2_, `/scratch` directory is the place to store the OpenMPI session.
While launching MPI applications, if this directory doesn't exists or it's not writable, you must specify other using the `TMPDIR` environment variable.

[source,shell]
    $ module purge
    $ module load singularity/2.3.2
    $ export TMPDIR=/tmp
    $ singularity exec -B /scratch:/tmp $CONTAINER $COMMAND


== 3. Shared-Memory containers

=== 3.1 Bootstrap

There isn't any known particular restriction for running shared-memory applications from a _Singularity_ container.

=== 3.2 Usage

[source,shell]
    $ module purge
    $ module load singularity/2.3.2
    $ singularity exec -B /scratch -B /mnt $CONTAINER $COMMAND

== 4. MPI containers

=== 4.1 Bootstrap

In order to run parallel applications in multiple nodes, _Singularity_ documentation tell us that the container must support MPI and PMI(x). Also for taking advantage of some HPC resouces like Infiniband networks, the container must suppport it. This means that the container must have installed the propper libraries to communicate with the hardware and also to perform inter-process communications.

In the case of Infiniband, there is not any known restrictions about the infiniband libraries installed inside the container.

Regarding MPI, in the current context, due to the _Singularity_ hybrid MPI approach, you need to have the same implementation and version of MPI installed at the host and inside the container to run parallel/MPI applications. We strongly recommend to use an OpenMPI implementation.

Current installed MPI implementations/versions at _Finis Terrae II_ are:

.OpenMPI:
  * openmpi/1.10.2
  * openmpi/2.0.0
  * openmpi/2.0.1
  * openmpi/2.0.2
  * openmpi/2.1.1

.IntelMPI:

  * impi/5.1
  * impi/2017

.BullMPI:

  * bullxmpi/1.2.9.1

NOTE: You can get more info about how to load this modules using `module spider` tool.

=== 4.2 Usage

Please, take care of selecting the same MPI implementation and version at the host and inside the container.
You must use `mpirun`, instead of `srun`, as process manager to ensure PMI compatibility.


[source,shell]
    $ module purge
    $ module load $COMPILER $MPI_VERSION
    $ module load singularity/2.3.2
    $ mpirun $ARGS singularity exec -B /scratch -B /mnt $CONTAINER $COMMAND

== 5. GPU containers

=== 5.1 Bootstrap

In the particular case of using GPUs from a container, the contained NVidia driver must exactly match the NVidia driver installed at the host.
There are several alternatives in order to have the right NVidia driver within the container.

* Install it persistently inside the container.
* Bind-mount the host driver inside the container.

NOTE: The big con of a persistent installation is the lack of portability, as you cannot use the same container in other host with a different NVidia driver version.

=== 5.2 Usage

_Singularity_ provides the `--nv` option to automagically bind-mount the NVidia drivers (experimental Nvidia support).

NOTE: Please, ensure that you are in a GPU compute node to run your GPU containers.

[source,shell]
    $ module purge
    $ module load singularity/2.3.2
    $ mpirun singularity exec --nv -B /scratch -B /mnt $CONTAINER $COMMAND

== 6. Bootstrap sample files

Some templates stored in this https://github.com/MSO4SC/Singularity[github repository]

=== 6.1 Basic bootstrap template

https://github.com/MSO4SC/Singularity/blob/master/examples/bootstrap_basic_template.def[Basic bootstrap template]

=== 6.2 MPI bootstrap template

https://github.com/MSO4SC/Singularity/blob/master/examples/bootstrap_mpi_template.def[MPI bootstrap template]
